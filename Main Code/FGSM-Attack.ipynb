{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eaae14d-3350-4c1a-a454-f0f7319bb822",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392a2f0b-7eb8-4c68-9300-d582bd194b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import network\n",
    "import argparse\n",
    "import platform\n",
    "import ivtmetrics # You must \"pip install ivtmetrics\" to use\n",
    "import dataloader\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "# import lpips\n",
    "import piq\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a18d72e-37a8-4c0b-baf5-323eb953dd0a",
   "metadata": {},
   "source": [
    "# Argument Parser\n",
    "\n",
    "## In case of running using cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c801f3f-6f0e-4f82-8ee0-638898728424",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "!export CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87d0903c-107a-41d8-ad36-21e8e6434a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% @args parsing\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--model', type=str, default='rendezvous', choices=['rendezvous'], help='Model name?')\n",
    "parser.add_argument('--version', type=int, default=0,  help='Model version control (for keeping several versions)') \n",
    "parser.add_argument('--hr_output', action='store_true', help='Whether to use higher resolution output (32x56) or not (8x14). Default: False')\n",
    "parser.add_argument('--use_ln', action='store_true', help='Whether to use layer norm or batch norm in AddNorm() function. Default: False')\n",
    "parser.add_argument('--decoder_layer', type=int, default=8, help='Number of MHMA layers ') \n",
    "# job\n",
    "parser.add_argument('-t', '--train', action='store_true', help='to train.')\n",
    "parser.add_argument('-e', '--test',  action='store_true', help='to test')\n",
    "parser.add_argument('--val_interval', type=int, default=1,  help='(for hp tuning). Epoch interval to evaluate on validation data. set -1 for only after final epoch, or a number higher than the total epochs to not validate.')\n",
    "# data\n",
    "parser.add_argument('--data_dir', type=str, default='/path/to/dataset', help='path to dataset?')\n",
    "parser.add_argument('--dataset_variant', type=str, default='cholect45-crossval', choices=['cholect50', 'cholect45', 'cholect50-challenge', 'cholect50-crossval', 'cholect45-crossval'], help='Variant of the dataset to use')\n",
    "parser.add_argument('-k', '--kfold', type=int, default=1,  choices=[1,2,3,4,5,], help='The test split in k-fold cross-validation')\n",
    "parser.add_argument('--image_width', type=int, default=448, help='Image width ')  \n",
    "parser.add_argument('--image_height', type=int, default=256, help='Image height ')  \n",
    "parser.add_argument('--image_channel', type=int, default=3, help='Image channels ')  \n",
    "parser.add_argument('--num_tool_classes', type=int, default=6, help='Number of tool categories')\n",
    "parser.add_argument('--num_verb_classes', type=int, default=10, help='Number of verb categories')\n",
    "parser.add_argument('--num_target_classes', type=int, default=15, help='Number of target categories')\n",
    "parser.add_argument('--num_triplet_classes', type=int, default=100, help='Number of triplet categories')\n",
    "parser.add_argument('--augmentation_list', type=str, nargs='*', default=['original', 'vflip', 'hflip', 'contrast', 'rot90'], help='List augumentation styles (see dataloader.py for list of supported styles).')\n",
    "# hp\n",
    "parser.add_argument('-b', '--batch', type=int, default=32,  help='The size of sample training batch')\n",
    "parser.add_argument('--epochs', type=int, default=100,  help='How many training epochs?')\n",
    "parser.add_argument('-w', '--warmups', type=int, nargs='+', default=[9,18,58], help='List warmup epochs for tool, verb-target, triplet respectively')\n",
    "parser.add_argument('-l', '--initial_learning_rates', type=float, nargs='+', default=[0.01, 0.01, 0.01], help='List learning rates for tool, verb-target, triplet respectively')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-5,  help='L2 regularization weight decay constant')\n",
    "parser.add_argument('--decay_steps', type=int, default=10,  help='Step to exponentially decay')\n",
    "parser.add_argument('--decay_rate', type=float, default=0.99,  help='Learning rates weight decay rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.95,  help=\"Optimizer's momentum\")\n",
    "parser.add_argument('--power', type=float, default=0.1,  help='Learning rates weight decay power')\n",
    "# weights\n",
    "parser.add_argument('--pretrain_dir', type=str, default='', help='path to pretrain_weight?')\n",
    "parser.add_argument('--test_ckpt', type=str, default=None, help='path to model weight for testing')\n",
    "# device\n",
    "parser.add_argument('--gpu', type=str, default=\"0\",  help='The gpu device to use. To use multiple gpu put all the device ids comma-separated, e.g: \"0,1,2\" ')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c479dfe0-a712-4ed5-ba76-cbe948f3f4de",
   "metadata": {},
   "source": [
    "# Parameters Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a6c2e5a-de5a-4caf-af2b-5b033622fe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring network ...\n"
     ]
    }
   ],
   "source": [
    "#%% @params definitions\n",
    "is_train        = FLAGS.train\n",
    "is_test         = FLAGS.test\n",
    "dataset_variant = FLAGS.dataset_variant\n",
    "data_dir        = FLAGS.data_dir\n",
    "kfold           = FLAGS.kfold if \"crossval\" in dataset_variant else 0\n",
    "version         = FLAGS.version\n",
    "hr_output       = FLAGS.hr_output\n",
    "use_ln          = FLAGS.use_ln\n",
    "batch_size      = FLAGS.batch\n",
    "pretrain_dir    = FLAGS.pretrain_dir\n",
    "test_ckpt       = FLAGS.test_ckpt\n",
    "weight_decay    = FLAGS.weight_decay\n",
    "learning_rates  = FLAGS.initial_learning_rates\n",
    "warmups         = FLAGS.warmups\n",
    "decay_steps     = FLAGS.decay_steps\n",
    "decay_rate      = FLAGS.decay_rate\n",
    "power           = FLAGS.power\n",
    "momentum        = FLAGS.momentum\n",
    "epochs          = FLAGS.epochs\n",
    "gpu             = FLAGS.gpu\n",
    "image_height    = FLAGS.image_height\n",
    "image_width     = FLAGS.image_width\n",
    "image_channel   = FLAGS.image_channel\n",
    "num_triplet     = FLAGS.num_triplet_classes\n",
    "num_tool        = FLAGS.num_tool_classes\n",
    "num_verb        = FLAGS.num_verb_classes\n",
    "num_target      = FLAGS.num_target_classes\n",
    "val_interval    = FLAGS.epochs-1 if FLAGS.val_interval==-1 else FLAGS.val_interval\n",
    "set_chlg_eval   = True if \"challenge\" in dataset_variant else False # To observe challenge evaluation protocol\n",
    "gpu             = \",\".join(str(FLAGS.gpu).split(\",\"))\n",
    "decodelayer     = FLAGS.decoder_layer\n",
    "addnorm         = \"layer\" if use_ln else \"batch\"\n",
    "modelsize       = \"high\" if hr_output else \"low\"\n",
    "FLAGS.multigpu  = len(gpu) > 1  # not yet implemented !\n",
    "mheaders        = [\"\",\"l\", \"cholect\", \"k\"]\n",
    "margs           = [FLAGS.model, decodelayer, dataset_variant, kfold]\n",
    "wheaders        = [\"norm\", \"res\"]\n",
    "wargs           = [addnorm, modelsize]\n",
    "modelname       = \"_\".join([\"{}{}\".format(x,y) for x,y in zip(mheaders, margs) if len(str(y))])+\"_\"+\\\n",
    "                  \"_\".join([\"{}{}\".format(x,y) for x,y in zip(wargs, wheaders) if len(str(x))])\n",
    "model_dir       = \"./__checkpoint__/run_{}\".format(version)\n",
    "\n",
    "if not os.path.exists(model_dir): os.makedirs(model_dir)\n",
    "resume_ckpt     = None\n",
    "ckpt_path       = os.path.join(model_dir, '{}.pth'.format(modelname))\n",
    "logfile         = os.path.join(model_dir, '{}.log'.format(modelname))\n",
    "data_augmentations      = FLAGS.augmentation_list \n",
    "iterable_augmentations  = []\n",
    "print(\"Configuring network ...\")\n",
    "\n",
    "#%% @functions (helpers)\n",
    "def assign_gpu(gpu=None):  \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu) \n",
    "    os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1' \n",
    "    \n",
    "\n",
    "def get_weight_balancing(case='cholect50'):\n",
    "    # 50:   cholecT50, data splits as used in rendezvous paper\n",
    "    # 50ch: cholecT50, data splits as used in CholecTriplet challenge\n",
    "    # 45cv: cholecT45, official data splits (cross-val)\n",
    "    # 50cv: cholecT50, official data splits (cross-val)\n",
    "    switcher = {\n",
    "        'cholect50': {\n",
    "            'tool'  :   [0.08084519, 0.81435289, 0.10459284, 2.55976864, 1.630372490, 1.29528455],\n",
    "            'verb'  :   [0.31956735, 0.07252306, 0.08111481, 0.81137309, 1.302895320, 2.12264151, 1.54109589, 8.86363636, 12.13692946, 0.40462028],\n",
    "            'target':   [0.06246232, 1.00000000, 0.34266478, 0.84750219, 14.80102041, 8.73795181, 1.52845100, 5.74455446, 0.285756500, 12.72368421, 0.6250808,  3.85771277, 6.95683453, 0.84923888, 0.40130032]\n",
    "        },\n",
    "        'cholect50-challenge': {\n",
    "            'tool':     [0.08495163, 0.88782288, 0.11259564, 2.61948830, 1.784866470, 1.144624170],\n",
    "            'verb':     [0.39862805, 0.06981640, 0.08332925, 0.81876204, 1.415868390, 2.269359150, 1.28428410, 7.35822511, 18.67857143, 0.45704490],\n",
    "            'target':   [0.07333818, 0.87139287, 0.42853950, 1.00000000, 17.67281106, 13.94545455, 1.44880997, 6.04889590, 0.326188650, 16.82017544, 0.63577586, 6.79964539, 6.19547658, 0.96284208, 0.51559559]\n",
    "        },\n",
    "        'cholect45-crossval': {\n",
    "            1: {\n",
    "                'tool':     [0.08165644, 0.91226868, 0.10674758, 2.85418156, 1.60554885, 1.10640067],\n",
    "                'verb':     [0.37870137, 0.06836869, 0.07931255, 0.84780024, 1.21880342, 2.52836879, 1.30765704, 6.88888889, 17.07784431, 0.45241117],\n",
    "                'target':   [0.07149629, 1.0, 0.41013597, 0.90458015, 13.06299213, 12.06545455, 1.5213205, 5.04255319, 0.35808332, 45.45205479, 0.67493897, 7.04458599, 9.14049587, 0.97330595, 0.52633249]\n",
    "                },\n",
    "            2: {\n",
    "                'tool':     [0.0854156, 0.89535362, 0.10995253, 2.74936869, 1.78264429, 1.13234529],\n",
    "                'verb':     [0.36346863, 0.06771776, 0.07893261, 0.82842725, 1.33892161, 2.13049748, 1.26120359, 5.72674419, 19.7, 0.43189126],\n",
    "                'target':   [0.07530655, 0.97961957, 0.4325135, 0.99393438, 15.5387931, 14.5951417, 1.53862569, 6.01836394, 0.35184462, 15.81140351, 0.709506, 5.79581994, 8.08295964, 1.0, 0.52689272]\n",
    "            },\n",
    "            3: {\n",
    "                \"tool\" :   [0.0915228, 0.89714969, 0.12057004, 2.72128174, 1.94092281, 1.12948557],\n",
    "                \"verb\" :   [0.43636862, 0.07558554, 0.0891017, 0.81820519, 1.53645582, 2.31924198, 1.28565657, 6.49387755, 18.28735632, 0.48676763],\n",
    "                \"target\" : [0.06841828, 0.90980736, 0.38826607, 1.0, 14.3640553, 12.9875, 1.25939394, 5.38341969, 0.29060227, 13.67105263, 0.59168565, 6.58985201, 5.72977941, 0.86824513, 0.47682423]\n",
    "\n",
    "            },\n",
    "            4: {\n",
    "                'tool':     [0.08222218, 0.85414117, 0.10948695, 2.50868784, 1.63235867, 1.20593318],\n",
    "                'verb':     [0.41154261, 0.0692142, 0.08427214, 0.79895288, 1.33625219, 2.2624166, 1.35343681, 7.63, 17.84795322, 0.43970609],\n",
    "                'target':   [0.07536126, 0.85398445, 0.4085784, 0.95464422, 15.90497738, 18.5978836, 1.55875831, 5.52672956, 0.33700863, 15.41666667, 0.74755423, 5.4921875, 6.11304348, 1.0, 0.50641118],\n",
    "            },\n",
    "            5: {\n",
    "                'tool':     [0.0804654, 0.92271157, 0.10489631, 2.52302243, 1.60074906, 1.09141982],\n",
    "                'verb':     [0.50710436, 0.06590258, 0.07981184, 0.81538866, 1.29267277, 2.20525568, 1.29699248, 7.32311321, 25.45081967, 0.46733895],\n",
    "                'target':   [0.07119395, 0.87450495, 0.43043372, 0.86465981, 14.01984127, 23.7114094, 1.47577277, 5.81085526, 0.32129865, 22.79354839, 0.63304067, 6.92745098, 5.88833333, 1.0, 0.53175798]\n",
    "            }\n",
    "        },\n",
    "        'cholect50-crossval': {\n",
    "            1:{\n",
    "                'tool':     [0.0828851, 0.8876, 0.10830995, 2.93907285, 1.63884786, 1.14499484],\n",
    "                'verb':     [0.29628942, 0.07366916, 0.08267971, 0.83155428, 1.25402434, 2.38358209, 1.34938741, 7.56872038, 12.98373984, 0.41502079],\n",
    "                'target':   [0.06551745, 1.0, 0.36345711, 0.82434783, 13.06299213, 8.61818182, 1.4017744, 4.62116992, 0.32822238, 45.45205479, 0.67343211, 4.13200498, 8.23325062, 0.88527215, 0.43113306],\n",
    "\n",
    "            },\n",
    "            2:{\n",
    "                'tool':     [0.08586283, 0.87716737, 0.11068887, 2.84210526, 1.81016949, 1.16283571],\n",
    "                'verb':     [0.30072757, 0.07275414, 0.08350168, 0.80694143, 1.39209979, 2.22754491, 1.31448763, 6.38931298, 13.89211618, 0.39397505],\n",
    "                'target':   [0.07056703, 1.0, 0.39451115, 0.91977006, 15.86206897, 9.68421053, 1.44483706, 5.44378698, 0.31858714, 16.14035088, 0.7238395, 4.20571429, 7.98264642, 0.91360477, 0.43304307],\n",
    "            },\n",
    "            3:{\n",
    "            'tool':      [0.09225068, 0.87856006, 0.12195811, 2.82669323, 1.97710987, 1.1603972],\n",
    "                'verb':     [0.34285159, 0.08049804, 0.0928239, 0.80685714, 1.56125608, 2.23984772, 1.31471136, 7.08835341, 12.17241379, 0.43180428],\n",
    "                'target':   [0.06919395, 1.0, 0.37532866, 0.9830703, 15.78801843, 8.99212598, 1.27597765, 5.36990596, 0.29177312, 15.02631579, 0.64935557, 5.08308605, 5.86643836, 0.86580743, 0.41908257], \n",
    "            },\n",
    "            4:{\n",
    "                'tool':     [0.08247885, 0.83095539, 0.11050268, 2.58193042, 1.64497676, 1.25538881],\n",
    "                'verb':     [0.31890981, 0.07380354, 0.08804592, 0.79094077, 1.35928144, 2.17017208, 1.42947103, 8.34558824, 13.19767442, 0.40666428],\n",
    "                'target':   [0.07777646, 0.95894072, 0.41993829, 0.95592153, 17.85972851, 12.49050633, 1.65701092, 5.74526929, 0.33763901, 17.31140351, 0.83747083, 3.95490982, 6.57833333, 1.0, 0.47139615],\n",
    "            },\n",
    "            5:{\n",
    "                'tool':     [0.07891691, 0.89878025, 0.10267677, 2.53805556, 1.60636428, 1.12691169],\n",
    "                'verb':     [0.36420961, 0.06825313, 0.08060635, 0.80956984, 1.30757221, 2.09375, 1.33625848, 7.9009434, 14.1350211, 0.41429631],\n",
    "                'target':   [0.07300329, 0.97128713, 0.42084942, 0.8829883, 15.57142857, 19.42574257, 1.56521739, 5.86547085, 0.32732733, 25.31612903, 0.70171674, 4.55220418, 6.13125, 1.0, 0.48528321],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return switcher.get(case)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ed47b-0c74-4f22-b693-3f5f74514db2",
   "metadata": {},
   "source": [
    "# Loading and Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b346c61f-cdd8-46df-a26b-53b730a700e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umairnawaz/.conda/envs/myenv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/umairnawaz/.conda/envs/myenv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built ...\n"
     ]
    }
   ],
   "source": [
    "# Path to model checkpoint\n",
    "test_ckpt = 'weights/rendezvous_l8_cholectcholect45-crossval_k1_batchnorm_lowres_180.pth'\n",
    "\n",
    "# Load base structure model\n",
    "model = network.Rendezvous('resnet18', hr_output=hr_output, use_ln=use_ln).cuda()\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "#%% performance tracker for hp tuning\n",
    "benchmark   = torch.nn.Parameter(torch.tensor([0.0]), requires_grad=False)\n",
    "print(\"Model built ...\")\n",
    "\n",
    "# Load the checkpoint of the trained model\n",
    "if os.path.exists(test_ckpt):\n",
    "    model.load_state_dict(torch.load(test_ckpt) ,  strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99121f68-aab1-41a4-b965-77fe339897f3",
   "metadata": {},
   "source": [
    "# Define Loss, Activation, and mAP Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f56a35e-c1cf-4d83-98e2-7c734c433445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics built ...\n"
     ]
    }
   ],
   "source": [
    "# Or constant weights from average of the random sampling of the dataset: we found this to produce better result.\n",
    "tool_weight     = [0.93487068, 0.94234964, 0.93487068, 1.18448115, 1.02368339, 0.97974447]\n",
    "verb_weight     = [0.60002400, 0.60002400, 0.60002400, 0.61682467, 0.67082683, 0.80163207, 0.70562823, 2.11208448, 2.69230769, 0.60062402]\n",
    "target_weight   = [0.49752894, 0.52041527, 0.49752894, 0.51394739, 2.71899565, 1.75577963, 0.58509403, 1.25228034, 0.49752894, 2.42993134, 0.49802647, 0.87266576, 1.36074165, 0.50150917, 0.49802647]\n",
    "\n",
    "\n",
    "#%% Loss\n",
    "activation  = nn.Sigmoid()\n",
    "loss_fn_i   = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(tool_weight).cuda())\n",
    "loss_fn_v   = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(verb_weight).cuda())\n",
    "loss_fn_t   = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(target_weight).cuda())\n",
    "loss_fn_ivt = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#Monkey patches\n",
    "np.float = float    \n",
    "np.int = int\n",
    "np.object = object\n",
    "np.bool = bool  \n",
    "\n",
    "#%% evaluation metrics\n",
    "mAP = ivtmetrics.Recognition(100)\n",
    "mAP.reset_global()\n",
    "if not set_chlg_eval:\n",
    "    mAPi = ivtmetrics.Recognition(6)\n",
    "    mAPv = ivtmetrics.Recognition(10)\n",
    "    mAPt = ivtmetrics.Recognition(15)\n",
    "    mAPi.reset_global()\n",
    "    mAPv.reset_global()\n",
    "    mAPt.reset_global()\n",
    "\n",
    "#%% Adversarial metrics\n",
    "mAP_adv = ivtmetrics.Recognition(100)\n",
    "mAP_adv.reset_global()\n",
    "if not set_chlg_eval:\n",
    "    mAPi_adv = ivtmetrics.Recognition(6)\n",
    "    mAPv_adv = ivtmetrics.Recognition(10)\n",
    "    mAPt_adv = ivtmetrics.Recognition(15)\n",
    "    mAPi_adv.reset_global()\n",
    "    mAPv_adv.reset_global()\n",
    "    mAPt_adv.reset_global()\n",
    "print(\"Metrics built ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6fe92-1b2d-49d6-b742-4b1ea06666b4",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeacc7d1-7acf-41df-929b-8d7d50bc4ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% data loading : variant and split selection (Note: original paper used different augumentation per epoch)\n",
    "data_dir = '/share/sdb/umairnawaz/Data/'\n",
    "dataset_variant= 'cholect45-crossval'\n",
    "dataset = dataloader.CholecT50( \n",
    "            dataset_dir=data_dir, \n",
    "            dataset_variant=dataset_variant,\n",
    "            test_fold=kfold,\n",
    "            augmentation_list=data_augmentations,\n",
    "            )\n",
    "\n",
    "# build dataset\n",
    "train_dataset, val_dataset, test_dataset = dataset.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c453d4cd-112c-4acf-9910-241d82a8bc18",
   "metadata": {},
   "source": [
    "# Load only test dataset in dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9bb56a-d89c-479d-a29e-b4877cfd973f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded ...\n"
     ]
    }
   ],
   "source": [
    "test_dataloaders = []\n",
    "for video_dataset in test_dataset:\n",
    "    test_dataloader = DataLoader(video_dataset, batch_size=batch_size, shuffle=False, prefetch_factor=3*batch_size, num_workers=3, pin_memory=True, persistent_workers=True, drop_last=False)\n",
    "    test_dataloaders.append(test_dataloader)\n",
    "print(\"Dataset loaded ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a24f9444-ad7c-468e-8439-b2e8a31e74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(val_list):\n",
    "    min_val = min(val_list)\n",
    "    max_val = max(val_list)\n",
    "\n",
    "    return (min_val, max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3029dfa7-a194-4ffb-a620-7252bf284315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to load the model on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to perform FGSM attack\n",
    "def fast_gradient_sign_method(model, x, y, eps=8/255):    \n",
    "\n",
    "    # Original Images\n",
    "    x = x.clone().detach().cuda().requires_grad_(True)\n",
    "    \n",
    "    targeted = False  # Change to True if we have a targeted attack\n",
    "    y_target = None    # Set the target label if it's a targeted attack\n",
    "\n",
    "    # Forward pass\n",
    "    tool, verb, target, triplet = model(x)\n",
    "\n",
    "    # Extracting Logits and CAMs of each category\n",
    "    \n",
    "    cam_i, logit_i  = tool\n",
    "    cam_v, logit_v  = verb\n",
    "    cam_t, logit_t  = target\n",
    "    logit_ivt       = triplet    \n",
    "\n",
    "    print(\"Image Shape: \" , x.shape)\n",
    "    \n",
    "    print(\"CAM I: \" , cam_i.shape)\n",
    "    print(\"CAM V: \" , cam_v.shape)\n",
    "    print(\"CAM T: \" , cam_t.shape)\n",
    "\n",
    "    print(\"I\" , cam_i[0][0].min().cpu().data.numpy() , \"I\" , cam_i[0][0].max().cpu().data.numpy())\n",
    "    print(\"V\" , cam_v[0][0].min().cpu().data.numpy() , \"V\" , cam_v[0][0].max().cpu().data.numpy())\n",
    "    print(\"T\" , cam_t[0][0].min().cpu().data.numpy() , \"I\" , cam_t[0][0].max().cpu().data.numpy())\n",
    "\n",
    "    # save_image(cam_i, f'Images-Attack/cam.png')\n",
    "\n",
    "    # Assuming cam_output is your CAM output and input_image is your input image\n",
    "    # You should adapt this based on your actual variable names and data format\n",
    "\n",
    "    # Load the input image\n",
    "    \n",
    "    # Define a normalization transform if needed\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    # Preprocess the input image (resize, normalize, convert to tensor)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Adjust size based on your model input size\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    \n",
    "    input_tensor = preprocess(x[0].detach().cpu().data.numpy())\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Assuming cam_output is a torch tensor with the same spatial dimensions as the input image\n",
    "    # You should adapt this based on your actual CAM output format\n",
    "    \n",
    "    # Resize CAM to match the input image size\n",
    "    cam_output_resized = torch.nn.functional.interpolate(cam_i.unsqueeze(0).unsqueeze(0), size=input_tensor.shape[1:], mode=\"bilinear\", align_corners=False)\n",
    "    \n",
    "    # Convert the CAM output to a numpy array\n",
    "    cam_array = cam_output_resized.squeeze(0).squeeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    # Normalize the CAM values to be between 0 and 1\n",
    "    cam_array = (cam_array - np.min(cam_array)) / (np.max(cam_array) - np.min(cam_array))\n",
    "\n",
    "    # Create a colormap (hot) to represent the heatmap\n",
    "    cmap = plt.cm.get_cmap(\"hot\")\n",
    "    \n",
    "    # Apply the colormap to the CAM array\n",
    "    heatmap = cmap(cam_array)\n",
    "    \n",
    "    # Overlay the heatmap on the input image\n",
    "    overlayed_image = heatmap[:, :, :3]  # Use only the RGB channels\n",
    "    overlayed_image = (overlayed_image * 255).astype(np.uint8)\n",
    "    \n",
    "    # Plot the original image and the overlayed image\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].imshow(input_image)\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[1].imshow(input_image)\n",
    "    ax[1].imshow(overlayed_image, alpha=0.7)  # Adjust alpha for transparency\n",
    "    ax[1].set_title(\"CAM Overlay\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(j)\n",
    "    # Compute Loss\n",
    "    loss_i          = loss_fn_i(logit_i, y[0].float())\n",
    "    loss_v          = loss_fn_v(logit_v, y[1].float())\n",
    "    loss_t          = loss_fn_t(logit_t, y[2].float())\n",
    "    loss_ivt        = loss_fn_ivt(logit_ivt, y[3].float()) \n",
    "\n",
    "    # Total Loss\n",
    "    loss = (loss_i) + (loss_v) + (loss_t) + loss_ivt \n",
    "\n",
    "    # Find the gradient\n",
    "    grad = torch.autograd.grad(\n",
    "        loss, x, retain_graph=False, create_graph=False, allow_unused=True\n",
    "    )[0]\n",
    "\n",
    "    # Update adversarial images\n",
    "    if grad is not None:\n",
    "        # print(\"Varifying Gradient\")\n",
    "        x_adv = x + (eps * grad.sign())\n",
    "        x_adv = torch.clamp(x_adv, min=0, max=1).detach()\n",
    "\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41ddb564-057b-4e5f-ba64-dee7c3a8db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization layer to normalize the input to expected input of model\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Add additional layer to normalize the input to expected input shape of model\n",
    "norm_model = nn.Sequential(\n",
    "            normalize,\n",
    "            model\n",
    "        ).to(device)\n",
    "norm_model = norm_model.eval()\n",
    "\n",
    "# Apply attack on the test dataloader\n",
    "def test_adv(dataloader, activation, final_eval=False):\n",
    "    # Reset the metrics\n",
    "    # Normal model metrics\n",
    "    mAP.reset()  \n",
    "    if final_eval and not set_chlg_eval:\n",
    "        mAPv.reset() \n",
    "        mAPt.reset() \n",
    "        mAPi.reset()\n",
    "    \n",
    "    # Adversarial Metrics\n",
    "    mAP_adv.reset()  \n",
    "    if final_eval and not set_chlg_eval:\n",
    "        mAPv_adv.reset() \n",
    "        mAPt_adv.reset() \n",
    "        mAPi_adv.reset()\n",
    "\n",
    "    \n",
    "    print(\"DataLoader Started:\")\n",
    "    for batch, (img, (y1, y2, y3, y4)) in enumerate(dataloader):\n",
    "        # Extract data in form of batches\n",
    "        img, y1, y2, y3, y4 = img.cuda(), y1.cuda(), y2.cuda(), y3.cuda(), y4.cuda()\n",
    "        \n",
    "        \n",
    "        #################### Normal Testing #####################\n",
    "        # Predictions of the model on clean images\n",
    "        tool, verb, target, triplet = norm_model(img)\n",
    "\n",
    "        # Final Evaluation \n",
    "        if final_eval:\n",
    "            # Get logits and CAMs for each individual category\n",
    "            cam_i, logit_i = tool\n",
    "            cam_v, logit_v = verb\n",
    "            cam_t, logit_t = target\n",
    "\n",
    "            # Update the mAP metrics for each prediction of clean images\n",
    "            mAPi.update(y1.float().detach().cpu(), activation(logit_i).detach().cpu()) # Log metrics \n",
    "            mAPv.update(y2.float().detach().cpu(), activation(logit_v).detach().cpu()) # Log metrics \n",
    "            mAPt.update(y3.float().detach().cpu(), activation(logit_t).detach().cpu()) # Log metrics \n",
    "        mAP.update(y4.float().detach().cpu(), activation(triplet).detach().cpu()) # Log metrics \n",
    "\n",
    "\n",
    "        ############# Adversarial Attack Started ##############\n",
    "\n",
    "        # Adversarial images obtained from FGSM\n",
    "        adversarial_example = fast_gradient_sign_method(norm_model, img, (y1, y2, y3, y4), eps)\n",
    "\n",
    "        # Save the clean and adversarial images\n",
    "        for i , img_adv in enumerate(adversarial_example):\n",
    "            save_image(img[i], f'Images-Attack/Clean/img{i}.png')\n",
    "            save_image(img_adv, f'Images-Attack/Adv/img-adv{i}.png')\n",
    "            i += 1\n",
    "            if i == 5:\n",
    "                print(j)\n",
    "\n",
    "        #### Finding Metrics for Image Quality ####\n",
    "\n",
    "        psnr_values_list.append(piq.psnr(adversarial_example, img).item())\n",
    "        ssim_values_list.append(piq.ssim(adversarial_example, img).item())\n",
    "        lpips_values_list.append(lpips(2*adversarial_example-1, 2*img-1).item())\n",
    "\n",
    "        ######################## Adversarial Testing ########################\n",
    "        \n",
    "        # Testing the attacked images by the model\n",
    "        tool_adv, verb_adv, target_adv, triplet_adv = norm_model(adversarial_example)\n",
    "        \n",
    "        if final_eval:\n",
    "            # Update the mAP metrics for each prediction of adversarial images using logits\n",
    "            \n",
    "            cam_i, logit_i_adv = tool_adv\n",
    "            cam_v, logit_v_adv = verb_adv\n",
    "            cam_t, logit_t_adv = target_adv\n",
    "            mAPi_adv.update(y1.float().detach().cpu(), activation(logit_i_adv).detach().cpu()) # Log metrics \n",
    "            mAPv_adv.update(y2.float().detach().cpu(), activation(logit_v_adv).detach().cpu()) # Log metrics \n",
    "            mAPt_adv.update(y3.float().detach().cpu(), activation(logit_t_adv).detach().cpu()) # Log metrics \n",
    "        mAP_adv.update(y4.float().detach().cpu(), activation(triplet_adv).detach().cpu()) # Log metrics \n",
    "\n",
    "    # End the update of each metric\n",
    "    if final_eval:\n",
    "        mAPv.video_end()\n",
    "        mAPt.video_end()\n",
    "        mAPi.video_end()\n",
    "        \n",
    "    if final_eval:\n",
    "        mAPv_adv.video_end()\n",
    "        mAPt_adv.video_end()\n",
    "        mAPi_adv.video_end()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a068fbc-0ad1-4615-a76f-32f5c6a85934",
   "metadata": {},
   "source": [
    "# Main function for applying attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a541c24-423d-42ec-81ac-32bc2cf33fd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment started ...\n",
      "   logging outputs to:  weights/Logs/FGSM-Attack.log\n",
      "DataLoader Started:\n",
      "Image Shape:  torch.Size([32, 3, 256, 448])\n",
      "CAM I:  torch.Size([32, 6, 8, 14])\n",
      "CAM V:  torch.Size([32, 10, 8, 14])\n",
      "CAM T:  torch.Size([32, 15, 8, 14])\n",
      "I -42.215252 I -4.679444\n",
      "V -10.202584 V -4.211567\n",
      "T -10.599937 I -3.2322445\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Load each dataloader iteratively\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_dataloader \u001b[38;5;129;01min\u001b[39;00m test_dataloaders:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Pass the loader to the testing function\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mtest_adv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone...!. Printing Stats.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Compute the final update after each dataloader\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m####### Normal Metrics #####\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 56\u001b[0m, in \u001b[0;36mtest_adv\u001b[0;34m(dataloader, activation, final_eval)\u001b[0m\n\u001b[1;32m     50\u001b[0m mAP\u001b[38;5;241m.\u001b[39mupdate(y4\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(), activation(triplet)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()) \u001b[38;5;66;03m# Log metrics \u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m############# Adversarial Attack Started ##############\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Adversarial images obtained from FGSM\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m adversarial_example \u001b[38;5;241m=\u001b[39m \u001b[43mfast_gradient_sign_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my4\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Save the clean and adversarial images\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i , img_adv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(adversarial_example):\n",
      "Cell \u001b[0;32mIn[34], line 50\u001b[0m, in \u001b[0;36mfast_gradient_sign_method\u001b[0;34m(model, x, y, eps)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Preprocess the input image (resize, normalize, convert to tensor)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     45\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),  \u001b[38;5;66;03m# Adjust size based on your model input size\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     47\u001b[0m     normalize,\n\u001b[1;32m     48\u001b[0m ])\n\u001b[0;32m---> 50\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m input_batch \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Assuming cam_output is a torch tensor with the same spatial dimensions as the input image\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# You should adapt this based on your actual CAM output format\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Resize CAM to match the input image size\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.9/site-packages/torchvision/transforms/functional.py:476\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    472\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    474\u001b[0m         )\n\u001b[0;32m--> 476\u001b[0m _, image_height, image_width \u001b[38;5;241m=\u001b[39m \u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    478\u001b[0m     size \u001b[38;5;241m=\u001b[39m [size]\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.9/site-packages/torchvision/transforms/functional.py:78\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:31\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     29\u001b[0m     width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [channels, height, width]\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "################### Perform FGSM attack ##################\n",
    "\n",
    "# Set FGSM parameters\n",
    "eps = 8/255\n",
    "exp_number = 1\n",
    "\n",
    "# Logs Saving File\n",
    "version_adv = 'FGSM-Attack'\n",
    "logfile  = f'weights/Logs/{version_adv}.log'\n",
    "\n",
    "#%% log config\n",
    "header1 = \"** Experiment for FGSM Attack **\"\n",
    "header2 = \"** Eps: {}/255 \\t Exp Number: {} **\".format(int(eps * 255) , exp_number)\n",
    "# header3 = \"** LR Config: Init: {} | Peak: {} | Warmup Epoch: {} | Rise: {} | Decay {} | train params {} | all params {} **\".format([float(f'{sch.get_last_lr()[0]:.6f}') for sch in lr_schedulers], [float(f'{v:.6f}') for v in wp_lr], warmups, power, decay_rate, pytorch_train_params, pytorch_total_params)\n",
    "maxlen  = len(header1)\n",
    "# header1 = \"{}{}{}\".format('*'*((maxlen-len(header1))//2+1), header1, '*'*((maxlen-len(header1))//2+1) )\n",
    "# header2 = \"{}{}{}\".format('*'*((maxlen-len(header2))//2+1), header2, '*'*((maxlen-len(header2))//2+1) )\n",
    "# header3 = \"{}{}{}\".format('*'*((maxlen-len(header3))//2+1), header3, '*'*((maxlen-len(header3))//2+1) )\n",
    "# maxlen  = max(len(header1), len(header2), len(header3))\n",
    "print(\"\\n\\n\\n{}\\n{}\\n{}\\n{}\\n\\n\".format(\"*\"*maxlen, header1, header2, \"*\"*maxlen), file=open(logfile, 'a+'))\n",
    "print(\"Experiment started ...\\n   logging outputs to: \", logfile)\n",
    "\n",
    "# Reset the metrics globally\n",
    "mAP.reset_global()\n",
    "mAP_adv.reset_global()\n",
    "\n",
    "mAPi.reset_global()\n",
    "mAPv.reset_global()\n",
    "mAPt.reset_global()\n",
    "\n",
    "mAPi_adv.reset_global()\n",
    "mAPv_adv.reset_global()\n",
    "mAPt_adv.reset_global()\n",
    "\n",
    "# Create a list to store image quality metrics for each batch\n",
    "psnr_values_list = []\n",
    "ssim_values_list = []\n",
    "lpips_values_list = []\n",
    "lpips = piq.LPIPS()\n",
    "\n",
    "# Load each dataloader iteratively\n",
    "for test_dataloader in test_dataloaders:\n",
    "    # Pass the loader to the testing function\n",
    "    test_adv(test_dataloader, activation, final_eval=True)\n",
    "    \n",
    "    print(\"Done...!. Printing Stats.\")\n",
    "\n",
    "# Compute the final update after each dataloader\n",
    "\n",
    "####### Normal Metrics #####\n",
    "\n",
    "if set_chlg_eval:\n",
    "    mAP_i = mAP.compute_video_AP('i', ignore_null=set_chlg_eval)\n",
    "    mAP_v = mAP.compute_video_AP('v', ignore_null=set_chlg_eval)\n",
    "    mAP_t = mAP.compute_video_AP('t', ignore_null=set_chlg_eval)\n",
    "else:\n",
    "    mAP_i = mAPi.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "    mAP_v = mAPv.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "    mAP_t = mAPt.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "\n",
    "mAP_iv = mAP.compute_video_AP('iv', ignore_null=set_chlg_eval)\n",
    "mAP_it = mAP.compute_video_AP('it', ignore_null=set_chlg_eval)\n",
    "mAP_ivt = mAP.compute_video_AP('ivt', ignore_null=set_chlg_eval) \n",
    "\n",
    "####### Adversarial Metrics #####\n",
    "\n",
    "if set_chlg_eval:\n",
    "    mAP_i_adv = mAP_adv.compute_video_AP('i', ignore_null=set_chlg_eval)\n",
    "    mAP_v_adv = mAP_adv.compute_video_AP('v', ignore_null=set_chlg_eval)\n",
    "    mAP_t_adv = mAP_adv.compute_video_AP('t', ignore_null=set_chlg_eval)\n",
    "else:\n",
    "    mAP_i_adv = mAPi_adv.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "    mAP_v_adv = mAPv_adv.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "    mAP_t_adv = mAPt_adv.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "\n",
    "mAP_iv_adv = mAP_adv.compute_video_AP('iv', ignore_null=set_chlg_eval)\n",
    "mAP_it_adv = mAP_adv.compute_video_AP('it', ignore_null=set_chlg_eval)\n",
    "mAP_ivt_adv = mAP_adv.compute_video_AP('ivt', ignore_null=set_chlg_eval) \n",
    "\n",
    "# Print the results into the logging file\n",
    "\n",
    "######### Printing Baseline Model Stats #########\n",
    "\n",
    "print('-'*50, file=open(logfile, 'a+'))\n",
    "print('Test Results\\nPer-category AP: ', file=open(logfile, 'a+'))\n",
    "print('-'*50, file=open(logfile, 'a+'))\n",
    "print(f'Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT ', file=open(logfile, 'a+'))\n",
    "print(f':::::: : {mAP_i[\"mAP\"]:.4f} | {mAP_v[\"mAP\"]:.4f} | {mAP_t[\"mAP\"]:.4f} | {mAP_iv[\"mAP\"]:.4f} | {mAP_it[\"mAP\"]:.4f} | {mAP_ivt[\"mAP\"]:.4f} ', file=open(logfile, 'a+'))\n",
    "print('='*50, file=open(logfile, 'a+'))\n",
    "print(\"Test results saved @ \", logfile)\n",
    "\n",
    "######### Printing Adversarial Stats #########\n",
    "\n",
    "print('-'*50, file=open(logfile, 'a+'))\n",
    "print('Adversarial Test Results\\nPer-category AP: ', file=open(logfile, 'a+'))\n",
    "print('-'*50, file=open(logfile, 'a+'))\n",
    "print(f'Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT ', file=open(logfile, 'a+'))\n",
    "print(f':::::: : {mAP_i_adv[\"mAP\"]:.4f} | {mAP_v_adv[\"mAP\"]:.4f} | {mAP_t_adv[\"mAP\"]:.4f} | {mAP_iv_adv[\"mAP\"]:.4f} | {mAP_it_adv[\"mAP\"]:.4f} | {mAP_ivt_adv[\"mAP\"]:.4f} ', file=open(logfile, 'a+'))\n",
    "print('='*50, file=open(logfile, 'a+'))\n",
    "print('-'*50, file=open(logfile, 'a+'))\n",
    "print(f'PSNR: {np.mean(psnr_values_list):.4f}    | SSIM: {np.mean(ssim_values_list):.4f}     | LPIPS: {1 - np.mean(lpips_values_list):.4f}', file=open(logfile, 'a+'))\n",
    "print('-'*50, file=open(logfile, 'a+'))\n",
    "print(\"Test results saved @ \", logfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd711eb1-85ec-4a2c-ba02-d8fcd99071f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_list = [4,8,16,32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6e3846-3e11-4a60-b89d-2305dc4017d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Perform FGSM attack using the hyper params ##################\n",
    "for eps in eps_list:\n",
    "    # Set FGSM parameters\n",
    "    eps = eps/255\n",
    "    exp_number = 1\n",
    "    \n",
    "    # Logs Saving File\n",
    "    version_adv = 'FGSM-Attack'\n",
    "    logfile  = f'weights/LogsTooba/{version_adv}.log'\n",
    "    \n",
    "    #%% log config\n",
    "    header1 = \"** Experiment for FGSM Attack **\"\n",
    "    header2 = \"** Eps: {}/255 **\".format(int(eps * 255) )\n",
    "    maxlen  = len(header1)\n",
    "\n",
    "    print(\"\\n\\n\\n{}\\n{}\\n{}\\n{}\\n\\n\".format(\"*\"*maxlen, header1, header2, \"*\"*maxlen), file=open(logfile, 'a+'))\n",
    "    print(\"Experiment started ...\\n   logging outputs to: \", logfile)\n",
    "    \n",
    "    mAP.reset_global()\n",
    "    mAP_adv.reset_global()\n",
    "    \n",
    "    mAPi.reset_global()\n",
    "    mAPv.reset_global()\n",
    "    mAPt.reset_global()\n",
    "    \n",
    "    mAPi_adv.reset_global()\n",
    "    mAPv_adv.reset_global()\n",
    "    mAPt_adv.reset_global()\n",
    "\n",
    "    psnr_values_list = []\n",
    "    ssim_values_list = []\n",
    "    lpips_values_list = []\n",
    "    lpips = piq.LPIPS()\n",
    "    \n",
    "    for test_dataloader in test_dataloaders:\n",
    "        # index = 1\n",
    "        test_adv(test_dataloader, model, activation, final_eval=True)\n",
    "        \n",
    "        # break\n",
    "        ###### Normal ######\n",
    "        print(\"Done...!. Printing Stats.\")\n",
    "    if set_chlg_eval:\n",
    "        mAP_i = mAP.compute_video_AP('i', ignore_null=set_chlg_eval)\n",
    "        mAP_v = mAP.compute_video_AP('v', ignore_null=set_chlg_eval)\n",
    "        mAP_t = mAP.compute_video_AP('t', ignore_null=set_chlg_eval)\n",
    "    else:\n",
    "        mAP_i = mAPi.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "        mAP_v = mAPv.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "        mAP_t = mAPt.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "    \n",
    "    mAP_iv = mAP.compute_video_AP('iv', ignore_null=set_chlg_eval)\n",
    "    mAP_it = mAP.compute_video_AP('it', ignore_null=set_chlg_eval)\n",
    "    mAP_ivt = mAP.compute_video_AP('ivt', ignore_null=set_chlg_eval) \n",
    "    \n",
    "    ####### Adversarial Metrics #####\n",
    "    \n",
    "    if set_chlg_eval:\n",
    "        mAP_i_adv = mAP_adv.compute_video_AP('i', ignore_null=set_chlg_eval)\n",
    "        mAP_v_adv = mAP_adv.compute_video_AP('v', ignore_null=set_chlg_eval)\n",
    "        mAP_t_adv = mAP_adv.compute_video_AP('t', ignore_null=set_chlg_eval)\n",
    "    else:\n",
    "        mAP_i_adv = mAPi_adv.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "        mAP_v_adv = mAPv_adv.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "        mAP_t_adv = mAPt_adv.compute_video_AP(ignore_null=set_chlg_eval)\n",
    "    \n",
    "    mAP_iv_adv = mAP_adv.compute_video_AP('iv', ignore_null=set_chlg_eval)\n",
    "    mAP_it_adv = mAP_adv.compute_video_AP('it', ignore_null=set_chlg_eval)\n",
    "    mAP_ivt_adv = mAP_adv.compute_video_AP('ivt', ignore_null=set_chlg_eval) \n",
    "    \n",
    "    print('-'*50, file=open(logfile, 'a+'))\n",
    "    print('Test Results: ', file=open(logfile, 'a+'))\n",
    "    print('-'*50, file=open(logfile, 'a+'))\n",
    "    print(f'Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT ', file=open(logfile, 'a+'))\n",
    "    print(f':::::: : {mAP_i[\"mAP\"]:.4f} | {mAP_v[\"mAP\"]:.4f} | {mAP_t[\"mAP\"]:.4f} | {mAP_iv[\"mAP\"]:.4f} | {mAP_it[\"mAP\"]:.4f} | {mAP_ivt[\"mAP\"]:.4f} ', file=open(logfile, 'a+'))\n",
    "    print('='*50, file=open(logfile, 'a+'))\n",
    "    print(\"Test results saved @ \", logfile)\n",
    "    \n",
    "    \n",
    "    print('-'*50, file=open(logfile, 'a+'))\n",
    "    print('Adversarial Test Results: ', file=open(logfile, 'a+'))\n",
    "    print('-'*50, file=open(logfile, 'a+'))\n",
    "    print(f'Mean AP:  I  |  V  |  T  |  IV  |  IT  |  IVT ', file=open(logfile, 'a+'))\n",
    "    print(f':::::: : {mAP_i_adv[\"mAP\"]:.4f} | {mAP_v_adv[\"mAP\"]:.4f} | {mAP_t_adv[\"mAP\"]:.4f} | {mAP_iv_adv[\"mAP\"]:.4f} | {mAP_it_adv[\"mAP\"]:.4f} | {mAP_ivt_adv[\"mAP\"]:.4f} ', file=open(logfile, 'a+'))\n",
    "    print('='*50, file=open(logfile, 'a+'))\n",
    "    print('-'*50, file=open(logfile, 'a+'))\n",
    "    print(f'PSNR: {np.mean(psnr_values_list):.4f}    | SSIM: {np.mean(ssim_values_list):.4f}     | LPIPS: {1 - np.mean(lpips_values_list):.4f}', file=open(logfile, 'a+'))\n",
    "    print('-'*50, file=open(logfile, 'a+'))\n",
    "    print(\"Test results saved @ \", logfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a957de9-bbd5-48e1-82fd-81a1d9792a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c156aac-af94-4eed-97a4-fe019dfc52e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
